ifndef::bookdir[]
:bookdir: _media/
endif::[]

:module_name: _module_01

ifdef::localmediadir[]
:imagesdir: {bookdir}{module_name}
endif::[]
ifndef::localmediadir[]
:imagesdir: _media
endif::[]

:img_dir: {imagesdir}

:toc: left
:toclevels: 4

== Data Science Project - WordleBot

== Project Overview

image::wordle.PNG[role=right, width=400] 

In teams, you will write a program capable of playing the popular game _WORDLE_ both manually and automatically. +

The Wordle guess we will be using will have custom weights involving solution selection, in both likelihood and number of possible solutions.

Should you choose to _borrow_ a WordleBot from someone's public GitHub, it will not preform optimally for this version of Wordle.

* This extends to YouTube tutorials on building WordleBots, information from Google or other resources relating to optimal Wordle play, etc.  

=== Requirements 

.*Ability to manually play a game of _WORDLE_:*

* Randomly selects a solution
* Accept user inputs for words
* Provide correct feedback to user after a guess
* Provide suggestions for good moves
** As determined with an appropriate application of Bayesian Statistics
* Inform users when they've guessed the correct word
* Inform users of the total number of guesses they used to solve the _WORDLE_

.*Ability to play an automated game of _WORDLE_:*

* Randomly selects a solution
* Solves the _WORDLE_ using a iterative method of guessing
** Using *_only_* the information which would be available to a human player
* Prints the number of guesses it took to solve the _WORDLE_
** Logs the individual guesses and the results strings returned to a file

*_Note:  No preclaculated data can be used after the 2nd guess_*

.*Ability to play several automated games of _WORDLE_:*

* Accept a ".txt" file
** Parse the file for lines consisting of only a valid 5 letter word
* Play an automated game of _WORDLE_ with every valid word in the file as the solution
* Print the average number of solves and time it took to solve them to the terminal
* Log the individual guesses for each of the words with the results strings returned for those guesses to a file

.*Presentation:*

* Groups will create a 15-20 minute presentation
** All group members will present and cover a section of the code they worked on
** Explain decisions for hardcoded vs dynamically determined logic within the program
** Explain word weights used for Word selection for _WORDLE_ games
** Explain how the next guess is determined when a game runs in an automated mode

* Groups will conduct a 10 minute demo of their project
** Project must complete a 100 _WORDLE_ puzzles from a .txt file during their demo
** Individual word guess and average number of guesses to solve must to be logged to a file

=== Suggested Features

.*Ability to generate a .txt file with random ordered solutions*

* The ".txt" file with 100 words for the presentation will be generated by the instructor WordleBot
** All groups will have the same list
* While not required for any task during this block, your bot should be able to generate a list in this fashion as well

=== Optional Features

* Color formatting during manual games
** Should closely mirror the traditional WORDLE feedback
* Documentation
** Both a users manual and an engineering design plan

=== Design Limitations

* Python
* 3rd party library limitations:
** Time, Math, random, argparse
** Pandas, JSON, CSV (or similar)
** colorama if doing color feature

** *_ANY_* libraries not included above must be individually approved by instructors

*_Note:_* _While it is true that the calculating the best possible next move could be sped up by computing optimization libraries and techniques (IE threading, async, etc.)._  

_These techniques are not in the spirit of the assignment (mathematical optimization vs CPU usage).  Additionally, techniques which utilize multiple threads but aren't spread across multiple cores are unlikely to provide significant speed increases as the calculations for best next moves are a CPU bound task._

=== Word Weights

Our version of wordle uses different rates for word selection than the traditional game.  

There are 12972 valid 5 letter words.  However, 2935 words are *_common solutions_*, which are the answers several years of the New York Times Wordle (going out several years in the future).

Solution selection will randomly select one of the 12972 valid wordle answers, but words on the *_common solutions_* list are *_10 times more likely_* to be the solution than valid words not on the list.

The valid words list can be found in "wordle_valid_words.txt" in the student resources.

The common words list can be found in the "world_common_words.txt" in the student resources.

.*_Code Example for Random Word Selection_*

[source, python]
----


def read_file_lines(filename):
    with open(filename, 'r') as file:
        lines = file.readlines()
    return [line.strip() for line in lines]

def pick_solution(word_list):
    from random import choice
    return choice(word_list)

valid_words = read_file_lines("wordle_valid_words.txt")
common_words = read_file_lines("wordle_common_words.txt")

valid_words.extend(common_words * 9)

solution = pick_solution(valid_words)
----

*_Note_*:  There are better ways to do construct data structures based on word weights, this is an oversimplified example intended to make word weights more intuitive.

== Program Flow

image::solution_flowchart.PNG[] 

There are several ways to approach the wordle problem.  Most will generally adhere to the flowchart above.  Regardless of the method choosen to analyze the dataset and claculate the best guess.  

The "Analyze Possible" block will vary in the steps it takes to determine the next guess.  We are going to cover several viable modeling methods for the Analysis of the dataset.

== Modeling Methods

=== Letter frequency

image::scrabble_e.JPG[role=right, width=400] 

Letter frequency analysis is a simple yet powerful technique that we can use to solve puzzles like Wordle. 

It's all about understanding how often each letter of the alphabet appears in a group of words or a piece of text. 

Imagine you have a giant bag of Scrabble tiles representing all the words in English. If you were to reach in and pull out a tile, it's more likely you'd pull out an 'E' or a 'T' than a 'Z' or a 'Q'. That's because 'E' and 'T' are used more frequently in English words. Letter frequency analysis is a way of quantifying this idea: we give each letter a score based on how often it shows up.

In the context of Wordle, we can use letter frequency analysis to help us make smart guesses. Suppose we have a list of possible five-letter words. We could start by guessing a word that uses the letters that appear most often in our list. If that guess is incorrect, we can use the feedback to update our list and guess again, always prioritizing the most common letters.

It's important to note that in this approach, we're looking at each letter in isolation rather than considering the word as a whole. We don't care about whether the letters form a common word or not. We're just interested in how often each letter appears.

The general approach to Data Science modeling, where we, as humans, have insight into the fact that we are not incorporating all the relevant information required to preform the task into the model, is a *_Naive_* or *_High Bias_* modeling approach.

High bias/naive alogirthms are refered to as *_Naive Bayes_* methods.  They use the "naive" assumption of idenpendence between every pair of features.  

*_Why adopt a Naive Approach?_*

If we know the letter frequency approach assumes that letter position, the other letters in the word, etc., have no impact on the validity of a guess, why use this approach?

Naive approaches are used frequently in data science applications, and they generally provide several benefits:

* Effective
** Despite their bias nature, naive approaches tend to be surprisingly accurate

* Simplicity
** Naive approaches are typically straightforward to understand and implement. 
** Don't require complex algorithms or advanced mathematical knowledge.

* Fast Computation
** Naive approaches often require less computational resources, especially on large datasets

* Interpretability
** The results from naive methods are often easy to interpret
** Important when you need to understand how the model is functioning

* Benchmarking
** Naive methods can serve as a solid baseline for comparison

* Resilent to Overfitting

* Scalability

[source, letter_frequency.py]
----

# Define the letter frequency.
# Frequency data taken from Samuel Morse's frequency analysis of letters in typing samples
# https://www3.nd.edu/~busiforc/handouts/cryptography/letterfrequencies.html

letter_frequency = {
    'E': 12000, 'T': 9000, 'A': 8000, 'I': 8000, 'N': 8000, 'O': 8000, 'S': 8000,
    'H': 6400, 'R': 6200, 'D': 4400, 'L': 4000, 'U': 3400, 'C': 3000, 'M': 3000,
    'F': 2500, 'W': 2000, 'Y': 2000, 'G': 1700, 'P': 1700, 'B': 1600,
    'V': 1200, 'K': 800, 'Q': 500, 'J': 400, 'X': 400, 'Z': 200
}

# Set of possible words
possible_words = ['apple', 'brain', 'siren', 'globe', 'speak', 'chair', 'flame', 'study', 'train', 'water']

def calculate_word_score(word, letter_frequency):
    score = 0
    for letter in word:
        score += letter_frequency[letter.upper()]
    return score 

def guess_word(possible_words, letter_frequency):
    max_score = 0
    best_guess = None

    for word in possible_words:
        score = calculate_word_score(word, letter_frequency)
        if score > max_score:
            max_score = score
            best_guess = word

    return best_guess, max_score

# Example usage
guessed_word, score = guess_word(possible_words, letter_frequency)
print(f"Best Word: {guessed_word.upper()} with a score of: {score}")

----

*_Output_*:
[source, bash]
----
Best Word: SIREN with a score of: 42200

----

This is a relatively simple approach to using letter frequency analysis to determine the best guess.

However, there are some issues with this approach that become obvious when applied to a larger data set.  

For example: "EERIE" is the highest scoring word in the dataset, with a score of 50200.  

*_Is EERIE a good initial Wordle guess?_*

*_Ways to improve_*:

* claculate_word_score function does not account for multiples of the same letter.  The function can be modified to iterate over the set of letters in a word rather than each letter in a word.

[source, python]
----
 for letter in set(word):
    score += letter_frequency[letter.upper()]
----

* Morse's work on letter frequency in the 1830s might not hold up today.  Language evolves over time.  

* Morse analyzed typing samples including words of various lengths, not the set of 5 letter words.

* This approach does not account for letter position within a word:

** *_STARE_* vs *_TARES_*

** *_SLATE_* vs *_SALET_*

** *_SNAIL_* vs *_NAILS_*

* This apporach does not account for the word weights we've established

=== N-Grams

.N-Grams
image::n_gram.PNG[role=right, width=500] 

N-grams can help us understand and predict sequences, especially when we're dealing with text. 

An N-gram is a contiguous sequence of N items from a given sample of text or speech. In simple terms, it's just a way of breaking up text into chunks that make sense together.

Suppose we're dealing with words: 

1-gram (or unigram) is just a single word, like 'apple'. 

A 2-gram (or bigram) is a pair of words that appear together, like 'green apple'. 

A 3-gram (or trigram) is a trio of words, like 'eat green apple'. 

We can go on to create 4-grams, 5-grams, and so on, depending on what we're interested in.

Let's apply this to letters in the context of Wordle:

_Rather than looking at individual letters in isolation, we could create 2-grams or 3-grams of letters._ 

For example: the word 'apple' could be split into the 2-grams 'ap', 'pp', 'pl', and 'le'. This approach captures the idea that some pairs of letters are more likely to appear together than others. For instance, 'th' is a common 2-gram in English, while 'tz' is not.

Using N-grams can offer several advantages over simple letter frequency analysis. For starters, it takes into account the context of each letter: where it appears in relation to other letters. This can provide a more accurate picture of the structure and patterns in the words we're guessing.

N-grams can help address some of the biases that might creep in with a letter frequency analysis. For example, if 'e' is a common letter in our list of possible words, but it's usually at the end of words rather than at the beginning, a simple letter frequency analysis might overestimate how useful 'e' is as a starting letter.

Using N-grams can allow us to capture patterns and dependencies between letters that can be crucial for making accurate guesses. For example, knowing that 'qu' is a common 2-gram in English can help us guess words that contain a 'q'.

*_Why adopt an N-gram Approach?_*

* Contextual Understanding
** N-grams capture the context by considering sequences of items (e.g., letters, words), which can lead to more accurate modeling

* Pattern Recognition
** N-grams can detect patterns and dependencies between consecutive items, which are often missed by naive methods

* Linguistic Structure
** For text data, N-grams can capture important linguistic structures (e.g., phrases, idioms), leading to a better understanding of the data

* Flexible Granularity
** The 'N' in N-grams can be adjusted to capture different levels of context. Bigger N-values can capture longer dependencies

* Robustness to Noise
** When dealing with text, N-grams can be more robust to spelling errors or variants than individual letter analysis

* Powerful Baseline
** Like naive methods, N-grams can provide a strong baseline for more complex models, giving context to their performance

* Versatility
** N-grams can be used with many types of data, not just text. They can be used to analyze sequences in various domains

* Scalability
** Despite their increased complexity compared to naive methods, N-grams are still quite scalable for large datasets

* Incremental Learning
** New N-grams can be incorporated into the model as new data comes in, allowing the model to learn and improve over time

* Explainability
** While more complex than naive models, N-gram models can still be relatively interpretable, as they operate on understandable units (the N-grams themselves)

[source, n_gram.py]
----
# Define the 2-gram frequency.
bigram_frequency = {
    'AP': 500, 'PP': 400, 'PL': 300, 'LE': 200,
    'BR': 600, 'RA': 700, 'AI': 800, 'IN': 900,
    'SI': 550, 'IR': 450, 'RE': 350, 'EN': 250,
    'GL': 650, 'LO': 750, 'OB': 850, 'BE': 950,
    'SP': 520, 'PE': 420, 'EA': 320, 'AK': 220,
    'CH': 620, 'HA': 720, 'RS': 820, 'ME': 210,
    'FL': 510, 'LA': 410, 'AM': 310, 'MB': 230,
    'ST': 610, 'TU': 710, 'UD': 810, 'DY': 910,
    'TR': 560, 'RO': 460, 'IT': 360, 'VE': 260,
    'WA': 630, 'AT': 730, 'TE': 830, 'ER': 930,
}


# Set of possible words
possible_words = ['apple', 'brain', 'siren', 'globe', 'speak', 'chair', 'flame', 'study', 'train', 'water']

def calculate_word_score(word, bigram_frequency):
    score = 0
    # Create 2-grams from the word
    bigrams = [word[i:i+2] for i in range(len(word)-1)]
    for bigram in bigrams:
        score += bigram_frequency.get(bigram.upper(), 0)  # Use get to avoid KeyError for unseen bigrams
    return score

def guess_word(possible_words, bigram_frequency):
    max_score = 0
    best_guess = None

    for word in possible_words:
        score = calculate_word_score(word, bigram_frequency)
        if score > max_score:
            max_score = score
            best_guess = word

    return best_guess, max_score

# Example usage
guessed_word, score = guess_word(possible_words, bigram_frequency)
print(f"Best Word: {guessed_word.upper()} with a score of: {score}")

----

*_Output_*:
[source, bash]
----
Best Word: GLOBE with a score of: 3200
----

This is a relatively simple implementation of word selection based on frequency analysis of various 2-grams within possible guesses.

There are several iterations on N-grams which would improve the preformance of N-grams as a method for solving Wordle:

* Considering Different Lengths of N-grams
** We could also consider 3-grams for better context representation. 
** EX: 'ING' might be a common ending for English words and might be more effective than the 3 letters alone
** 3-grams might be practicularly useful for the beginnings and endings of words

* Positional N-grams
** In the current approach, N-grams are not position-specific. 
** EX: 'LE' at the end of a word might be more common than 'LE' at the beginning. 

* Incorporating the specified word weights for our Wordle game
** Our current approach values eliminating all words equally when in reality they  have different probabilities of being the solution

=== Entropy

image::entropy.PNG[role=right, width=500] 

Entropy is a concept in information theory that gives us a measure of the uncertainty or randomness in a set of data. 

In simple terms, entropy quantifies the amount of "surprise" or "information" that comes from learning the outcome of a random event. 

A higher entropy implies more unpredictability, while a lower entropy means more certainty.

Let's relate this to our Wordle game:  the entropy of a guess is a measure of the remaining uncertainty about the solution after that guess is made. If we guess a word and the game tells us which letters are correct and in the right positions, we've gained some information and reduced our uncertainty about the solution. The entropy of the guess is a way of quantifying that reduction in uncertainty.

By calculating the entropy of each potential guess, we can make informed decisions about which guesses are most likely to reduce our uncertainty the most. 

In other words:  _we're looking for the guess that gives us the most information, or equivalently, reduces our entropy the most._

The concept of entropy can offer several advantages over simpler methods like letter frequency analysis or N-gram analysis:

* Takes into account the entire set of possible solutions
** Not just the frequency of individual letters or letter combinations. 
** Considers the overall structure and patterns in the set of words.

*_Entropy inherently deals with probabilities_*, and it naturally accounts for the fact that some words are more likely than others. This can help avoid biases that might arise in other methods. EX: A letter might be common in the set of possible words, but if it's always in the wrong position, guessing that letter might not reduce our uncertainty much.

By aiming to minimize entropy, we're essentially trying to learn as much as we can with each guess. 

This aligns well with the goal of Wordle, which is to guess the word in as few attempts as possible.

Entropy claculates the best expected information gained across all possible outcomes of that guess.

Here is a simple example of using Entropy to determine the best guess from a set of words:

[source, entropy.py]
----
import math

# Set of possible words
possible_words = ['apple', 'brain', 'siren', 'globe', 'speak', 'chair', 'flame', 'study', 'train', 'water']

def process_guess(solution, guess):
    result = ""
    # Write this functin yourself ;p

    return result

def calculate_guess_entropy(guess, possible_words):
    # Calculate the number of words that would be eliminated for each possible result
    result_counts = {}
    for word in possible_words:
        result = process_guess(word, guess)  # Use your existing function
        if result not in result_counts:
            result_counts[result] = 0
        result_counts[result] += 1
    
    # Calculate the entropy of this guess
    entropy = 0
    for count in result_counts.values():
        p = count / len(possible_words)
        entropy += -p * math.log2(p)
    
    return entropy

# Function to guess a word based on entropy reduction
def guess_word_entropy(possible_words):
    min_entropy = float('inf')
    best_guess = None

    for word in possible_words:
        entropy = calculate_guess_entropy(word, possible_words)
        if entropy < min_entropy:
            min_entropy = entropy
            best_guess = word

    return best_guess, min_entropy

----

*_Output_*:
[source, bash]
----
Best Word: STUDY with an entropy of: 1.7609640474436812

----

*_NOTE:* This example code will not run without a process_guess function which returns the wordle response string for any individual guess._

There are, however, some significant drawbacks and Entropy approach:

* Computationally Intensive
** Calculating entropy typically involves iterating over all possible outcomes and their probabilities
** More computationally expensive than simpler methods, especially when the set of possible outcomes is large

* Requires more information
** Our entropy example requires the output string from the wordle game for the potential guess against EVERY possible solution
** Will need logic to figure out the response for every guess against the possible solutions

* Complexity
** Entropy calculations can be harder to understand for those who are not as comfortable with mathematical concepts
** Explaining how the model works is more difficult

* Debugging
** More difficult to check the claculations made by the model as compared to frequency analysis or N-grams

There are several iterations on entropy which would improve the preformance of Entropy as a method for solving Wordle:

* Hybrid Models
** Similar to N-grams, entropy calculations can be combined with other approaches to create a more powerful model
** EX: You could use frequency analysis to select a subset of words and then use entropy analysis to select the best word within that subset

* Entropy Threshold
** Define an entropy threshold
** If a guess reaches this threshold, it could be selected without computing the entropy of the rest of the words, saving computation time

== Timeline and Extra Credit

=== Timeline

End of day 1:  Have manually playable Wordle game

Catch up code will be provided at the end of day for any groups which don't hit this Benchmarking

End of day 2:  Determination of starting word or words

End of day 3:  Presentation and speed/performance Benchmarking

=== Extra Credit

Implementation of more than 1 method of automated Wordle solves

* Benchmark performance of methods against each other